diff --git a/train.py b/train.py
index de2c111..1763c7e 100644
--- a/train.py
+++ b/train.py
@@ -21,6 +21,7 @@ import torch
 import transformers
 from torch.utils.data import Dataset
 from transformers import Trainer
+from modeling_llama import llama_model
 
 import utils
 
@@ -193,18 +194,22 @@ def train():
     parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
 
-    model = transformers.AutoModelForCausalLM.from_pretrained(
-        model_args.model_name_or_path,
-        cache_dir=training_args.cache_dir,
-    )
+    # model = transformers.AutoModelForCausalLM.from_pretrained(
+    #     model_args.model_name_or_path,
+    #     cache_dir=training_args.cache_dir,
+    # )
+
+    # tokenizer = transformers.AutoTokenizer.from_pretrained(
+    #     model_args.model_name_or_path,
+    #     cache_dir=training_args.cache_dir,
+    #     model_max_length=training_args.model_max_length,
+    #     padding_side="right",
+    #     use_fast=False,
+    # )
+
+    model, tokenizer = llama_model(lm=True)
+    model.gradient_checkpointing_enable()
 
-    tokenizer = transformers.AutoTokenizer.from_pretrained(
-        model_args.model_name_or_path,
-        cache_dir=training_args.cache_dir,
-        model_max_length=training_args.model_max_length,
-        padding_side="right",
-        use_fast=False,
-    )
     if tokenizer.pad_token is None:
         smart_tokenizer_and_embedding_resize(
             special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),
diff --git a/utils.py b/utils.py
index 0d47b5f..23fd82c 100644
--- a/utils.py
+++ b/utils.py
@@ -7,6 +7,9 @@ import sys
 import time
 import json
 from typing import Optional, Sequence, Union
+from typing import Dict
+
+dict = Dict
 
 import openai
 import tqdm
